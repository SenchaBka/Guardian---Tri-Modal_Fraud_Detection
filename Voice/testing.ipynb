{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8ee9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#File for testing components of Voice Stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb64c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ApiError",
     "evalue": "headers: {'date': 'Wed, 18 Feb 2026 22:01:19 GMT', 'server': 'uvicorn', 'content-length': '1121', 'content-type': 'application/json', 'access-control-allow-origin': '*', 'access-control-allow-headers': '*', 'access-control-allow-methods': 'POST, PATCH, OPTIONS, DELETE, GET, PUT', 'access-control-max-age': '600', 'strict-transport-security': 'max-age=1800;', 'x-trace-id': '18e96f44a592b7576c786837cf93357e', 'x-region': 'us-central1', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 400, body: {'detail': {'type': 'validation_error', 'code': 'invalid_parameters', 'message': \"Invalid language code received: 'None'. Please leave blank if you want to automatically detect the language or use one of: afr, amh, ara, asm, ast, aze, bak, bas, bel, ben, bhr, bod, bos, bre, bul, cat, ceb, ces, chv, ckb, cnh, cre, cym, dan, dav, deu, div, dyu, ell, eng, epo, est, eus, fao, fas, fil, fin, fra, fry, ful, gla, gle, glg, guj, hat, hau, heb, hin, hrv, hsb, hun, hye, ibo, ina, ind, isl, ita, jav, jpn, kab, kan, kas, kat, kaz, kea, khm, kin, kir, kln, kmr, kor, kur, lao, lat, lav, lij, lin, lit, ltg, ltz, lug, luo, mal, mar, mdf, mhr, mkd, mlg, mlt, mon, mri, mrj, msa, mya, myv, nan, nep, nhi, nld, nor, nso, nya, oci, ori, orm, oss, pan, pol, por, pus, quy, roh, ron, rus, sah, san, sat, sin, skr, slk, slv, smo, sna, snd, som, sot, spa, sqi, srd, srp, sun, swa, swe, tam, tat, tel, tgk, tha, tig, tir, tok, ton, tsn, tuk, tur, twi, uig, ukr, umb, urd, uzb, vie, vot, vro, wol, xho, yid, yor, yue, zgh, zho, zul, zza.\", 'status': 'invalid_language_code', 'request_id': '18e96f44a592b7576c786837cf93357e', 'param': 'language_code'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mApiError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m audio_path = \u001b[33m\"\u001b[39m\u001b[33mdata/sample.m4a\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(audio_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     transcription = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspeech_to_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscribe_v2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage_code\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mNone\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# set to None for auto-detect\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiarize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# speaker labels\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag_audio_events\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# laughter/applause/etc.\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(transcription)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsen\\OneDrive\\Desktop\\conuhacks\\Guardian---Tri-Modal_Fraud_Detection\\venv\\Lib\\site-packages\\elevenlabs\\speech_to_text\\client.py:152\u001b[39m, in \u001b[36mSpeechToTextClient.convert\u001b[39m\u001b[34m(self, model_id, enable_logging, file, language_code, tag_audio_events, num_speakers, timestamps_granularity, diarize, diarization_threshold, additional_formats, file_format, cloud_storage_url, webhook, webhook_id, temperature, seed, use_multi_channel, webhook_metadata, entity_detection, keyterms, request_options)\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     44\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     66\u001b[39m ) -> SpeechToTextConvertResponse:\n\u001b[32m     67\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[33;03m    Transcribe an audio or video file. If webhook is set to true, the request will be processed asynchronously and results sent to configured webhooks. When use_multi_channel is true and the provided audio has multiple channels, a 'transcripts' object with separate transcripts for each channel is returned. Otherwise, returns a single transcript. The optional webhook_metadata parameter allows you to attach custom data that will be included in webhook responses for request correlation and tracking.\u001b[39;00m\n\u001b[32m     69\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m    )\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raw_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable_logging\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlanguage_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtag_audio_events\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtag_audio_events\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_speakers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_speakers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimestamps_granularity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimestamps_granularity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiarize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiarize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdiarization_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiarization_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_formats\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfile_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcloud_storage_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcloud_storage_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwebhook\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebhook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwebhook_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebhook_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_multi_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_multi_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwebhook_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwebhook_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mentity_detection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentity_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeyterms\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeyterms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\arsen\\OneDrive\\Desktop\\conuhacks\\Guardian---Tri-Modal_Fraud_Detection\\venv\\Lib\\site-packages\\elevenlabs\\speech_to_text\\raw_client.py:191\u001b[39m, in \u001b[36mRawSpeechToTextClient.convert\u001b[39m\u001b[34m(self, model_id, enable_logging, file, language_code, tag_audio_events, num_speakers, timestamps_granularity, diarize, diarization_threshold, additional_formats, file_format, cloud_storage_url, webhook, webhook_id, temperature, seed, use_multi_channel, webhook_metadata, entity_detection, keyterms, request_options)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code=_response.status_code, headers=\u001b[38;5;28mdict\u001b[39m(_response.headers), body=_response.text)\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ApiError(status_code=_response.status_code, headers=\u001b[38;5;28mdict\u001b[39m(_response.headers), body=_response_json)\n",
      "\u001b[31mApiError\u001b[39m: headers: {'date': 'Wed, 18 Feb 2026 22:01:19 GMT', 'server': 'uvicorn', 'content-length': '1121', 'content-type': 'application/json', 'access-control-allow-origin': '*', 'access-control-allow-headers': '*', 'access-control-allow-methods': 'POST, PATCH, OPTIONS, DELETE, GET, PUT', 'access-control-max-age': '600', 'strict-transport-security': 'max-age=1800;', 'x-trace-id': '18e96f44a592b7576c786837cf93357e', 'x-region': 'us-central1', 'via': '1.1 google', 'alt-svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'}, status_code: 400, body: {'detail': {'type': 'validation_error', 'code': 'invalid_parameters', 'message': \"Invalid language code received: 'None'. Please leave blank if you want to automatically detect the language or use one of: afr, amh, ara, asm, ast, aze, bak, bas, bel, ben, bhr, bod, bos, bre, bul, cat, ceb, ces, chv, ckb, cnh, cre, cym, dan, dav, deu, div, dyu, ell, eng, epo, est, eus, fao, fas, fil, fin, fra, fry, ful, gla, gle, glg, guj, hat, hau, heb, hin, hrv, hsb, hun, hye, ibo, ina, ind, isl, ita, jav, jpn, kab, kan, kas, kat, kaz, kea, khm, kin, kir, kln, kmr, kor, kur, lao, lat, lav, lij, lin, lit, ltg, ltz, lug, luo, mal, mar, mdf, mhr, mkd, mlg, mlt, mon, mri, mrj, msa, mya, myv, nan, nep, nhi, nld, nor, nso, nya, oci, ori, orm, oss, pan, pol, por, pus, quy, roh, ron, rus, sah, san, sat, sin, skr, slk, slv, smo, sna, snd, som, sot, spa, sqi, srd, srp, sun, swa, swe, tam, tat, tel, tgk, tha, tig, tir, tok, ton, tsn, tuk, tur, twi, uig, ukr, umb, urd, uzb, vie, vot, vro, wol, xho, yid, yor, yue, zgh, zho, zul, zza.\", 'status': 'invalid_language_code', 'request_id': '18e96f44a592b7576c786837cf93357e', 'param': 'language_code'}}"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = ElevenLabs(api_key=os.getenv(\"ELEVENLABS_API_KEY\"))\n",
    "\n",
    "# Put a local audio file here (mp3/wav/m4a, etc.)\n",
    "audio_path = \"data/sample.m4a\"\n",
    "\n",
    "with open(audio_path, \"rb\") as f:\n",
    "    transcription = client.speech_to_text.convert(\n",
    "        file=f,\n",
    "        model_id=\"scribe_v2\",\n",
    "        language_code=None,       # set to None for auto-detect\n",
    "        diarize=True,              # speaker labels\n",
    "        tag_audio_events=True      # laughter/applause/etc.\n",
    "    )\n",
    "\n",
    "print(transcription)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
